# config/ssl/generative_augmentation.yaml
# This configuration file is for the Generative Augmentation FixMatch agent.
# It defines the hyperparameters, dataset, model, and other components needed for training.
# === General Configuration ===
log_dir: "./logs/generative_fixmatch"
run_name: run_{{date}}_{{num_labeled}}
description: Generative Augmentation FixMatch with Rectified Flow
author: Hyunseo Choung
date: "2025-10-23"

agent: generative_augmentation_fixmatch

# === Global Hyperparameters ===
epochs: 2
batch_size: 16
num_workers: 4
data_root: "./data/mstar"
size: 128
num_classes: 10
pretrained: false
padding: 16
padding_mode: reflect
# Normalization parameters for MSTAR dataset
mean: !!python/tuple [0.1414, 0.1414, 0.1414]
std: !!python/tuple [0.1258, 0.1258, 0.1258]
lr: 0.003
weight_decay: 0

# === Grid Search & Agent-specific Hyperparameters ===
steps_per_epoch: 128
num_labeled: 100

mu: 7
confidence_threshold: 0.95

# === Difficulty Computation ===
# Exponential scaling: s = ((C^a * exp(-a*H)) - 1) / (C^a - 1)
scale_a: 10.0

# === Loss Weights ===
# Classifier Stage
lambda_pl: 1.0  # Pseudo-label loss
lambda_cons: 0.5  # Consistency KL loss

# Generator Stage
lambda_rf: 1.0  # Rectified Flow velocity matching
lambda_ratio: 1.0  # Difficulty ratio lower bound
lambda_lpips: 1.0  # LPIPS perceptual upper bound
lambda_identity: 1.0  # Identity preservation (unconditional)
lambda_diff_eq: 1.0  # Difficulty equality (unconditional)

# === Difficulty Constraints ===
# Lower bound: log(s_strong/s_weak) >= 1/C (automatic, no config needed)
# Upper bound: LPIPS applied only when lower bound satisfied

# === Rectified Flow Configuration ===
ode_steps: 100  # ODE integration steps for sampling

# === Component Selections ===

# Models: Classifier & Generator
model:
  classifier: resnet18
  generator: conditional_unet(channels=3, dim=64, dim_mults=[1,2,4,8], dropout=0.1, num_residual_streams=2)

# Optimizers
optimizer:
  classifier: adam
  generator: adam(lr=0.0001)

# ODE Solver (from rectified_flow_trainer.py)
# Midpoint method with tight tolerances for accurate sampling
solver: ode_solver(method="midpoint", atol=1e-5, rtol=1e-5)

# === Data Pipeline ===

# Dataset: MSTAR SAR images
dataset:
  train: mstar(split="train")
  val: mstar(split="test")

# Dataloader: Basic DataLoader
dataloader:
  labeled: basic
  unlabeled: basic
  val: basic

# Sampler: Subset random sampling for SSL
sampler:
  labeled: subset_random
  unlabeled: subset_random

# === Transforms ===

# Weak augmentation: Minimal transformations
transform:
  weak: "resize | random_flip | random_crop | to_tensor | normalize"
  val: "resize | to_tensor | normalize"

# === Loss Functions ===

# Supervised: Cross-Entropy for classification
# Conditioning Upper Bound: LPIPS perceptual loss
loss:
  supervised: cross_entropy(reduction="mean")
  cond_upper: lpips

# === Metrics ===

# Validation: Classification accuracy
metric:
  val: accuracy

# === Logging ===

# CSV logger for metrics tracking
logger: csv

# === Notes ===
# 1. Generator learns identity mapping (x_0 = x_1 = weak)
# 2. Difficulty conditions deviation from identity
# 3. Batch split: First half conditional, second half unconditional
# 4. Two-stage training: Generator â†’ Classifier
# 5. Masking: PL uses m_cond*m_conf, Consistency uses m_cond only
# 6. All losses normalized by batch size B
# 7. Loss-driven variation: L_ratio, L_lpips, L_id, L_diff_eq create subtle deviations from identity
# 8. Null conditioning (difficulty=0) for unconditional generation
