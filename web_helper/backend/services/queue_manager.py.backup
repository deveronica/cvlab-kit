"""
Advanced queue management service
"""

import asyncio
import json
import logging
import subprocess
import uuid
import time
import os
import signal
import hashlib
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set
import threading
import queue
import psutil

from ..models.queue import (
    QueueJob, JobStatus, JobPriority, QueueStats,
    ResourceRequirement, QueueConfiguration, JobSubmission
)
from ..models import Device, get_db
from .event_manager import event_manager

logger = logging.getLogger(__name__)


def generate_experiment_uid() -> str:
    """Generate unique experiment UID in format {YYYYMMDD}_{hash4}"""
    date_str = datetime.now().strftime("%Y%m%d")
    random_str = str(random.randint(0, 999999))
    hash_hex = hashlib.md5(random_str.encode()).hexdigest()
    return f"{date_str}_{hash_hex[:4]}"


class QueueManager:
    """Advanced queue management system"""

    def __init__(self, config: Optional[QueueConfiguration] = None):
        self.config = config or QueueConfiguration()
        self._jobs: Dict[str, QueueJob] = {}
        self._job_queue = queue.PriorityQueue()
        self._running_jobs: Set[str] = set()
        self._device_assignments: Dict[str, List[str]] = {}  # device_id -> [job_ids]
        self._lock = threading.RLock()

        # Background worker thread
        self._worker_thread = None
        self._stop_event = threading.Event()
        self._started = False

        # Load persistent state
        self._load_state()

    def start(self):
        """Start the queue manager"""
        if self._started:
            return

        self._started = True
        self._stop_event.clear()
        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self._worker_thread.start()
        logger.info("Queue manager started")

    def stop(self):
        """Stop the queue manager"""
        if not self._started:
            return

        self._stop_event.set()
        if self._worker_thread:
            self._worker_thread.join(timeout=5.0)

        self._save_state()
        self._started = False
        logger.info("Queue manager stopped")

    def submit_job(self, submission: JobSubmission) -> QueueJob:
        """Submit a new job to the queue"""
        job_id = str(uuid.uuid4())

        # Use provided experiment_uid or generate new one
        experiment_uid = submission.experiment_uid or generate_experiment_uid()

        # Note: run_uid removed - cvlabkit handles run_name from config YAML

        job = QueueJob(
            job_id=job_id,
            experiment_uid=experiment_uid,
            name=submission.name,
            project=submission.project,
            config_path=submission.config_path,
            priority=submission.priority,
            requirements=submission.requirements,
            tags=submission.tags,
            environment_vars=submission.environment_vars,
            metadata=submission.metadata,
            command=self._build_command(submission.config_path),
            working_directory=str(Path.cwd())
        )

        with self._lock:
            self._jobs[job_id] = job

            # Add to priority queue (lower number = higher priority)
            priority_value = self._get_priority_value(job.priority)
            self._job_queue.put((priority_value, job.created_at.timestamp(), job_id))

            job.status = JobStatus.QUEUED
            job.queued_at = datetime.now()

        logger.info(f"Job {job_id} ({job.name}) submitted to queue")
        self._save_state()

        # Broadcast job submission
        self._broadcast_job_update_sync(job, "job_submitted")

        return job

    def get_job(self, job_id: str) -> Optional[QueueJob]:
        """Get job by ID"""
        return self._jobs.get(job_id)

    def list_jobs(self, status: Optional[JobStatus] = None,
                  project: Optional[str] = None) -> List[QueueJob]:
        """List jobs with optional filtering"""
        jobs = list(self._jobs.values())

        if status:
            jobs = [job for job in jobs if job.status == status]

        if project:
            jobs = [job for job in jobs if job.project == project]

        # Sort by creation time (newest first)
        jobs.sort(key=lambda j: j.created_at, reverse=True)
        return jobs

    def cancel_job(self, job_id: str) -> bool:
        """Cancel a job"""
        job = self._jobs.get(job_id)
        if not job:
            return False

        with self._lock:
            if job.status == JobStatus.RUNNING:
                # Try to terminate the process
                self._terminate_job(job_id)

            job.status = JobStatus.CANCELLED
            job.completed_at = datetime.now()
            self._free_device(job_id)

        logger.info(f"Job {job_id} cancelled")
        self._save_state()

        # Broadcast job cancellation
        self._broadcast_job_update_sync(job, "job_cancelled")

        return True

    def _worker_loop(self):
        """Main worker loop for processing jobs"""
        while not self._stop_event.is_set():
            try:
                self._process_queue()
                self._monitor_running_jobs()
                self._stop_event.wait(5.0)
            except Exception as e:
                logger.error(f"Error in queue worker loop: {e}", exc_info=True)
                self._stop_event.wait(10.0)

    def _process_queue(self):
        """Process queued jobs and start them if resources are available"""
        if len(self._running_jobs) >= self.config.max_concurrent_jobs:
            return

        available_devices = self._get_available_devices()
        if not available_devices:
            return

        while (not self._job_queue.empty() and
               len(self._running_jobs) < self.config.max_concurrent_jobs):
            try:
                _, _, job_id = self._job_queue.get_nowait()
                job = self._jobs.get(job_id)
                if not job or job.status != JobStatus.QUEUED:
                    continue

                device = self._find_suitable_device(job, available_devices)
                if not device:
                    self._job_queue.put((self._get_priority_value(job.priority), job.queued_at.timestamp(), job_id))
                    break

                if self._start_job(job, device):
                    available_devices.remove(device)
            except queue.Empty:
                break
            except Exception as e:
                logger.error(f"Error processing queue: {e}", exc_info=True)

    def _start_job(self, job: QueueJob, device: str) -> bool:
        """Start a job on the specified device as a detached process."""
        if os.name == 'nt':
            logger.error("Detached process execution is not supported on Windows.")
            return False
            
        try:
            env = os.environ.copy()
            if device.startswith('cuda:'):
                env['CUDA_VISIBLE_DEVICES'] = device.split(':')[1]
            elif device == 'cpu':
                env['CUDA_VISIBLE_DEVICES'] = ''
            if job.environment_vars:
                env.update(job.environment_vars)

            # Create experiment-specific log directory
            log_dir = Path("web_helper/queue_logs") / job.experiment_uid
            log_dir.mkdir(parents=True, exist_ok=True)

            # Standardized file names
            stdout_file = log_dir / "terminal_log.log"
            stderr_file = log_dir / "terminal_err.log"

            command = (
                f"nohup uv run main.py --config {job.config_path} "
                f"> {stdout_file} 2> {stderr_file} & echo $!"
            )

            process = subprocess.Popen(
                command,
                cwd=job.working_directory,
                env=env,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                preexec_fn=os.setsid
            )

            stdout, stderr = process.communicate()
            pid_str = stdout.strip().decode()
            if not pid_str.isdigit():
                raise RuntimeError(f"Could not get PID for job {job.job_id}. Stderr: {stderr.decode()}")
            
            pid = int(pid_str)

            if not job.metadata:
                job.metadata = {}
            job.metadata['stdout_log'] = str(stdout_file)
            job.metadata['stderr_log'] = str(stderr_file)
            job.metadata['pid'] = pid

            with self._lock:
                job.status = JobStatus.RUNNING
                job.started_at = datetime.now()
                job.assigned_device = device
                self._running_jobs.add(job.job_id)
                if device not in self._device_assignments:
                    self._device_assignments[device] = []
                self._device_assignments[device].append(job.job_id)

            logger.info(f"Started job {job.job_id} ({job.name}) on device {device} with PID {pid}")
            self._save_state()
            self._broadcast_job_update_sync(job, "job_started")

            threading.Thread(target=self._monitor_job, args=(job.job_id,), daemon=True).start()
            return True

        except Exception as e:
            logger.error(f"Failed to start job {job.job_id}: {e}", exc_info=True)
            with self._lock:
                job.status = JobStatus.FAILED
                job.error_message = f"Failed to start: {str(e)}"
                job.completed_at = datetime.now()
            return False

    def _monitor_job(self, job_id: str):
        """Monitor a specific job's execution using its PID."""
        job = self._jobs.get(job_id)
        if not job:
            return

        pid = job.metadata.get('pid')
        if not pid:
            logger.error(f"No PID found for monitoring job {job_id}")
            return

        try:
            time.sleep(2)

            while job.status == JobStatus.RUNNING:
                if not psutil.pid_exists(pid):
                    logger.info(f"Process with PID {pid} for job {job_id} no longer exists.")
                    with self._lock:
                        if job.status == JobStatus.RUNNING:
                            job.status = JobStatus.COMPLETED
                            job.progress = 1.0
                            job.completed_at = datetime.now()
                            if not job.metadata:
                                job.metadata = {}
                            job.metadata['completion_status'] = 'assumed_completed_by_monitor'
                            self._free_device(job_id)
                    
                    self._broadcast_job_update_sync(job, "job_completed")
                    self._save_state()
                    break
                
                self._update_job_progress(job)
                time.sleep(10)

        except Exception as e:
            logger.error(f"Error monitoring job {job_id}: {e}", exc_info=True)
            with self._lock:
                job.status = JobStatus.FAILED
                job.error_message = f"Monitoring error: {str(e)}"
                job.completed_at = datetime.now()
                self._free_device(job_id)
            self._broadcast_job_update_sync(job, "job_failed")

    def _monitor_running_jobs(self):
        pass

    def _terminate_job(self, job_id: str):
        """Terminate a running job using its PID."""
        job = self._jobs.get(job_id)
        if not job or not job.metadata or 'pid' not in job.metadata:
            logger.warning(f"Cannot terminate job {job_id}: no PID found")
            return

        pid = job.metadata['pid']
        try:
            if psutil.pid_exists(pid):
                process = psutil.Process(pid)
                for child in process.children(recursive=True):
                    child.terminate()
                process.terminate()
                
                gone, alive = psutil.wait_procs([process], timeout=3)
                if alive:
                    for p in alive:
                        p.kill()
                logger.info(f"Terminated job {job_id} (PID {pid})")
            else:
                logger.info(f"Process for job {job_id} (PID {pid}) already terminated.")
        except psutil.NoSuchProcess:
             logger.info(f"Process for job {job_id} (PID {pid}) already terminated.")
        except Exception as e:
            logger.error(f"Error terminating job {job_id}: {e}", exc_info=True)

    def _free_device(self, job_id: str):
        """Free the device assigned to a job"""
        with self._lock:
            self._running_jobs.discard(job_id)
            job = self._jobs.get(job_id)
            if job and job.assigned_device:
                device_jobs = self._device_assignments.get(job.assigned_device, [])
                if job_id in device_jobs:
                    device_jobs.remove(job_id)

    def _save_state(self):
        """Save queue state to disk"""
        try:
            state_file = Path("web_helper/queue_state.json")
            state_file.parent.mkdir(exist_ok=True)
            
            with self._lock:
                serializable_jobs = {}
                for job_id, job in self._jobs.items():
                    job_dict = job.model_dump(exclude_none=True)
                    for field in ['created_at', 'queued_at', 'started_at', 'completed_at']:
                        if job_dict.get(field):
                            job_dict[field] = job_dict[field].isoformat()
                    serializable_jobs[job_id] = job_dict

                state = {
                    "jobs": serializable_jobs,
                }

            with open(state_file, 'w') as f:
                json.dump(state, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save queue state: {e}", exc_info=True)

    def _load_state(self):
        """Load queue state and recover running jobs."""
        state_file = Path("web_helper/queue_state.json")
        if not state_file.exists():
            return

        try:
            with open(state_file, 'r') as f:
                state = json.load(f)

            for job_id, job_dict in state.get("jobs", {}).items():
                for field in ['created_at', 'queued_at', 'started_at', 'completed_at']:
                    if job_dict.get(field):
                        job_dict[field] = datetime.fromisoformat(job_dict[field])
                
                job = QueueJob(**job_dict)
                self._jobs[job_id] = job

                if job.status == JobStatus.RUNNING:
                    pid = job.metadata.get('pid')
                    if pid and psutil.pid_exists(pid):
                        logger.info(f"Recovering running job {job_id} with PID {pid}")
                        self._running_jobs.add(job_id)
                        if job.assigned_device:
                            if job.assigned_device not in self._device_assignments:
                                self._device_assignments[job.assigned_device] = []
                            self._device_assignments[job.assigned_device].append(job_id)
                        
                        threading.Thread(target=self._monitor_job, args=(job.job_id,), daemon=True).start()
                    else:
                        logger.warning(f"Job {job_id} was in running state but PID {pid} not found. Marking as failed.")
                        job.status = JobStatus.FAILED
                        job.error_message = "Process terminated while web_helper was offline."
                        job.completed_at = datetime.now()
                
                elif job.status == JobStatus.QUEUED:
                    priority_value = self._get_priority_value(job.priority)
                    self._job_queue.put((priority_value, job.created_at.timestamp(), job_id))

            logger.info(f"Loaded and processed {len(self._jobs)} jobs from state file")

        except Exception as e:
            logger.error(f"Failed to load queue state: {e}", exc_info=True)

    def _get_available_devices(self) -> List[str]:
        """Get list of available and healthy devices.

        Only returns devices that:
        1. Have sent a heartbeat within the last 3 seconds (healthy)
        2. Are not currently assigned to max concurrent jobs

        Returns:
            List of device host_ids that are available for job assignment
        """
        try:
            db = next(get_db())
            devices = db.query(Device).all()

            available = []
            now = datetime.utcnow()

            for device in devices:
                # Check if device has recent heartbeat (within 3 seconds)
                if not device.last_heartbeat:
                    continue

                time_diff = (now - device.last_heartbeat).total_seconds()
                if time_diff > 3:
                    # Device is stale or disconnected
                    continue

                # Device is healthy, check if it's available
                device_id = device.host_id
                assigned_jobs = self._device_assignments.get(device_id, [])

                # Filter out completed/cancelled jobs
                active_jobs = [
                    job_id for job_id in assigned_jobs
                    if job_id in self._jobs and self._jobs[job_id].status == JobStatus.RUNNING
                ]

                if len(active_jobs) < 1:  # Only one job per device
                    available.append(device_id)

            if not available:
                logger.debug("No healthy devices available for job dispatch")

            return available

        except Exception as e:
            logger.error(f"Error getting available devices: {e}", exc_info=True)
            return []

    def _find_suitable_device(self, job: QueueJob, available_devices: List[str]) -> Optional[str]:
        return available_devices[0] if available_devices else None

    def _get_priority_value(self, priority: JobPriority) -> int:
        return {JobPriority.URGENT: 0, JobPriority.HIGH: 1, JobPriority.NORMAL: 2, JobPriority.LOW: 3}.get(priority, 2)

    def _build_command(self, config_path: str) -> str:
        return f"uv run main.py --config {config_path}"
        
    def _broadcast_job_update_sync(self, job: QueueJob, event_type: str = "status_change"):
        try:
            # Check if we're already in an event loop
            loop = asyncio.get_running_loop()
            # If we are, schedule the coroutine to run in the background
            loop.create_task(self._broadcast_job_update(job, event_type))
        except RuntimeError:
            # No running event loop, so we can use asyncio.run()
            asyncio.run(self._broadcast_job_update(job, event_type))

    async def _broadcast_job_update(self, job: QueueJob, event_type: str = "status_change"):
        try:
            await event_manager.send_queue_update(job.model_dump(exclude_none=True))
        except Exception as e:
            logger.error(f"Failed to broadcast job update for {job.job_id}: {e}")
    
    def _update_job_progress(self, job: QueueJob):
        pass # Simplified

# Global queue manager instance
_queue_manager: Optional[QueueManager] = None

def get_queue_manager() -> QueueManager:
    """Get the global queue manager instance"""
    global _queue_manager
    if _queue_manager is None:
        _queue_manager = QueueManager()
        _queue_manager.start()
    return _queue_manager

def shutdown_queue_manager():
    """Shutdown the global queue manager"""
    global _queue_manager
    if _queue_manager:
        _queue_manager.stop()
        _queue_manager = None
